# -*- coding: utf-8 -*-
"""PDL Challenge 1 Task I.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZS3BqWRqcRorgeDsOwS3tLfl8qORDbzz
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from scipy.interpolate import BarycentricInterpolator
import time
import random

# Set seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)
random.seed(42)

# Parameters
N = 512  # Total number of points in our domain (as mentioned in the example)
input_dim = 1  # Dimension of input (x)
output_dim = 1  # Dimension of output (y)

# Function to generate a random barycentric interpolated function
def generate_barycentric_function(num_samples=50):
    # Full range of x values
    x_full = np.linspace(0, 1, N)

    # Selecting random sample points
    sample_indices = np.sort(np.random.choice(N, num_samples, replace=False))
    sample_x = x_full[sample_indices]

    # Generate random y values for our samples (between -1 and 1)
    sample_y = np.cos(2 * np.pi * sample_x)
    #sample_y = np.random.uniform(-1, 1, num_samples)

    # Create the barycentric interpolator
    interpolator = BarycentricInterpolator(sample_x, sample_y)

    # Get the full interpolated function
    y_full = interpolator(x_full)

    """absmax = np.max(np.abs(y_full))
    if absmax > 0:
      y_full = y_full / absmax"""

    return x_full, y_full, sample_x, sample_y, interpolator

# Define neural network architecture with variable depth and activation
class DNN(nn.Module):
    def __init__(self, input_dim, hidden_layers, output_dim, activation='relu'):
        super(DNN, self).__init__()

        # Dictionary of activation functions
        self.activations = {
            'relu': nn.ReLU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid(),
            'leaky_relu': nn.LeakyReLU(0.01),
            'elu': nn.ELU(),
            'selu': nn.SELU(),
            'gelu': nn.GELU(),
            'softplus': nn.Softplus()
        }

        # Create network architecture
        layers = []

        # Input layer
        layers.append(nn.Linear(input_dim, hidden_layers[0]))
        layers.append(self.activations[activation])

        # Hidden layers
        for i in range(len(hidden_layers) - 1):
            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))
            layers.append(self.activations[activation])

        # Output layer
        layers.append(nn.Linear(hidden_layers[-1], output_dim))

        # Combine all layers
        self.model = nn.Sequential(*layers)

        # Calculate total number of neurons/parameters
        self.total_neurons = sum(hidden_layers)
        self.total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

    def forward(self, x):
        return self.model(x)

    def print_info(self):
        print(f"Model architecture: {self.model}")
        print(f"Total neurons in hidden layers: {self.total_neurons}")
        print(f"Total parameters: {self.total_params}")

# Function to train the model
def train_model(model, x_train, y_train, epochs=10000, lr=0.001, early_stop_threshold=1e-6):
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    # Convert numpy arrays to PyTorch tensors
    x_tensor = torch.FloatTensor(x_train.reshape(-1, 1))
    y_tensor = torch.FloatTensor(y_train.reshape(-1, 1))

    losses = []
    best_loss = float('inf')
    patience_counter = 0
    patience = 100  # Number of epochs to wait before early stopping

    start_time = time.time()

    for epoch in range(epochs):
        # Forward pass
        y_pred = model(x_tensor)
        loss = criterion(y_pred, y_tensor)

        # Backward pass and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Store loss
        losses.append(loss.item())

        # Print progress
        if epoch % 1000 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item():.8f}')

        # Early stopping check
        if loss.item() < best_loss - early_stop_threshold:
            best_loss = loss.item()
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= patience and loss.item() < 1e-4:
            print(f'Early stopping at epoch {epoch}, Loss: {loss.item():.8f}')
            break

    training_time = time.time() - start_time
    print(f"Training completed in {training_time:.2f} seconds")
    return losses

# Function to evaluate model
def evaluate_model(model, x_eval, y_true):
    x_tensor = torch.FloatTensor(x_eval.reshape(-1, 1))
    with torch.no_grad():
        y_pred = model(x_tensor).numpy().flatten()

    mse = np.mean((y_pred - y_true)**2)
    return mse, y_pred

# Function to run an experiment with specific hyperparameters
def run_experiment(dataset_size, hidden_layers, activation='relu'):
    # Generate the barycentric interpolated function
    x_full, y_full, sample_x, sample_y, interpolator = generate_barycentric_function(num_samples=50)

    # Create dataset of size M
    idx = np.sort(np.random.choice(N, dataset_size, replace=False))
    x_train = x_full[idx]
    y_train = y_full[idx]

    # Create the model
    model = DNN(input_dim, hidden_layers, output_dim, activation)
    print(f"\n--- Experiment with dataset_size={dataset_size}, hidden_layers={hidden_layers}, activation={activation} ---")
    model.print_info()

    # Train the model
    losses = train_model(model, x_train, y_train)

    # Evaluate on training points
    train_mse, train_preds = evaluate_model(model, x_train, y_train)
    print(f"MSE on training points: {train_mse:.8f}")

    # Evaluate on all points
    full_mse, full_preds = evaluate_model(model, x_full, y_full)
    print(f"MSE on all points: {full_mse:.8f}")

    # Evaluate on unseen points (points that weren't used for training)
    unseen_idx = np.setdiff1d(np.arange(N), idx)
    x_unseen = x_full[unseen_idx]
    y_unseen = y_full[unseen_idx]
    unseen_mse, unseen_preds = evaluate_model(model, x_unseen, y_unseen)
    print(f"MSE on unseen points: {unseen_mse:.8f}")

    # Plotting
    plt.figure(figsize=(12, 6))

    # Plot original and predicted function
    plt.subplot(1, 2, 1)
    plt.plot(x_full, y_full, label='Original Function', linewidth=2)
    plt.plot(x_full, full_preds, '--', label='DNN Prediction', linewidth=2)
    plt.scatter(x_train, y_train, c='red', label='Training Points', s=20)
    plt.title(f'Function Reconstruction\nMSE={full_mse:.8f}')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True)

    # Plot training loss
    plt.subplot(1, 2, 2)
    plt.plot(losses)
    plt.title('Training Loss (MSE)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.yscale('log')
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(f'task1_M{dataset_size}_L{len(hidden_layers)}_A{activation}.png')
    plt.show()

    return model, train_mse, full_mse, unseen_mse, losses

# Compare different dataset sizes
def compare_dataset_sizes(sizes, hidden_layers, activation='relu'):
    results = {}
    for size in sizes:
        _, train_mse, full_mse, unseen_mse, _ = run_experiment(size, hidden_layers, activation)
        results[size] = {'train_mse': train_mse, 'full_mse': full_mse, 'unseen_mse': unseen_mse}

    # Plot results
    plt.figure(figsize=(10, 6))
    sizes_list = list(results.keys())
    train_mse_list = [results[size]['train_mse'] for size in sizes_list]
    full_mse_list = [results[size]['full_mse'] for size in sizes_list]
    unseen_mse_list = [results[size]['unseen_mse'] for size in sizes_list]

    plt.plot(sizes_list, train_mse_list, 'o-', label='Training MSE')
    plt.plot(sizes_list, full_mse_list, 's-', label='Full Function MSE')
    plt.plot(sizes_list, unseen_mse_list, '^-', label='Unseen Points MSE')
    plt.title(f'Effect of Dataset Size on MSE\nNetwork: {hidden_layers}, Activation: {activation}')
    plt.xlabel('Dataset Size (M)')
    plt.ylabel('MSE')
    plt.yscale('log')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'task1_dataset_comparison_{activation}.png')
    plt.show()

    return results

# Compare different network depths
def compare_network_depths(dataset_size, architectures, activation='relu'):
    results = {}
    for arch in architectures:
        arch_name = f"{len(arch)}x{arch[0]}"  # e.g., "3x128" for 3 layers of 128 neurons
        _, train_mse, full_mse, unseen_mse, _ = run_experiment(dataset_size, arch, activation)
        results[arch_name] = {'train_mse': train_mse, 'full_mse': full_mse, 'unseen_mse': unseen_mse, 'total_neurons': sum(arch)}

    # Plot results
    plt.figure(figsize=(10, 6))
    arch_names = list(results.keys())
    train_mse_list = [results[name]['train_mse'] for name in arch_names]
    full_mse_list = [results[name]['full_mse'] for name in arch_names]
    unseen_mse_list = [results[name]['unseen_mse'] for name in arch_names]

    plt.plot(arch_names, train_mse_list, 'o-', label='Training MSE')
    plt.plot(arch_names, full_mse_list, 's-', label='Full Function MSE')
    plt.plot(arch_names, unseen_mse_list, '^-', label='Unseen Points MSE')
    plt.title(f'Effect of Network Depth on MSE\nDataset Size: {dataset_size}, Activation: {activation}')
    plt.xlabel('Network Architecture')
    plt.ylabel('MSE')
    plt.yscale('log')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'task1_depth_comparison_M{dataset_size}_{activation}.png')
    plt.show()

    return results

# Compare different activation functions
def compare_activations(dataset_size, hidden_layers, activations=['relu', 'tanh', 'sigmoid', 'leaky_relu', 'elu']):
    results = {}
    for activation in activations:
        _, train_mse, full_mse, unseen_mse, _ = run_experiment(dataset_size, hidden_layers, activation)
        results[activation] = {'train_mse': train_mse, 'full_mse': full_mse, 'unseen_mse': unseen_mse}

    # Plot results
    plt.figure(figsize=(10, 6))
    activation_names = list(results.keys())
    train_mse_list = [results[name]['train_mse'] for name in activation_names]
    full_mse_list = [results[name]['full_mse'] for name in activation_names]
    unseen_mse_list = [results[name]['unseen_mse'] for name in activation_names]

    x = np.arange(len(activation_names))
    width = 0.25

    plt.bar(x - width, train_mse_list, width, label='Training MSE')
    plt.bar(x, full_mse_list, width, label='Full Function MSE')
    plt.bar(x + width, unseen_mse_list, width, label='Unseen Points MSE')
    plt.yscale('log')
    plt.xticks(x, activation_names)
    plt.title(f'Effect of Activation Functions on MSE\nDataset Size: {dataset_size}, Network: {hidden_layers}')
    plt.xlabel('Activation Function')
    plt.ylabel('MSE (log scale)')
    plt.legend()
    plt.grid(True, axis='y')
    plt.savefig(f'task1_activation_comparison_M{dataset_size}.png')
    plt.show()

    return results

# Function to run all experiments
def run_all_experiments():
    # Test different dataset sizes
    dataset_sizes = [10, 20, 50, 100, 200]
    hidden_layers = [128, 128]  # 2 hidden layers with 128 neurons each
    dataset_results = compare_dataset_sizes(dataset_sizes, hidden_layers)

    # Test different network depths (staying within ~10,000 neuron budget)
    dataset_size = 100
    architectures = [
        [100],  # 1 hidden layer, 100 neurons
        [100, 100],  # 2 hidden layers, 100 neurons each
        [100, 100, 100],  # 3 hidden layers, 100 neurons each
        [500],  # 1 hidden layer, 500 neurons
        [250, 250],  # 2 hidden layers, 250 neurons each
        [100, 100, 100, 100, 100]  # 5 hidden layers, 100 neurons each
    ]
    depth_results = compare_network_depths(dataset_size, architectures)

    # Test different activation functions
    activations = ['relu', 'tanh', 'sigmoid', 'leaky_relu', 'elu', 'selu', 'gelu', 'softplus']
    activation_results = compare_activations(dataset_size, hidden_layers, activations)

    # Save results
    results = {
        "dataset_sizes": dataset_results,
        "network_depths": depth_results,
        "activations": activation_results
    }

    return results

# Run all experiments
if __name__ == "__main__":
    all_results = run_all_experiments()

    # Save the results to a file
    import json
    with open("experiment_results.json", "w") as f:
        json.dump(all_results, f, indent=4)

    print("All experiments completed and results saved.")